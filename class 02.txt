NN 구현 기초
보통 학습 데이터를 학습할 때, for문을 사용해 일일히 하나씩 학습하는 방식을 생각할탠데, 아니다.
또한 연산에 있어서 정방향 전파와 역방향 전파가 따로 존재한다.

로지스틱 회귀 == binary classification
결과의 0 or 1의 값으로 구분되는 방식


표기 
(x, y) == 하나의 훈련샘플 == (특성셋, 결과)
샘플 셋인 = X, 결과 = Y 는 행렬을 통해 표현되며 연산이 수월하다.

X = Nx (특성종류) * m(샘플개수)
이는 샘플 셋의 열을 샘플 개수 만큼 가지도록 함

Y = 1 * m
위 샘플 셋인 X와 이후 연산을 통한 값의 곱이 나오게 됨으로 이러한 형태를 가짐



Logistic Regression
출력될 레이블 y 가 0, 1 로 출력되는 이진 문제에 활용

입력 파라미터 x, 계수 w, 실수 b 의 식인 y = wx + b 이나,
0, 1 의 결과값으로 나오지 않는다.

이를 위해, 시그모이드 함수를 활용해 위 식을 0 or 1의 값으로 변경해 준다.
이를 위한 식이 G(z) = 1 / 1 + ^-z 이며, z 가 이전의 식인 y = wx + b 이다.
z 값에 따라 0 혹은 1 에 끝없이 가까워지는 형태가 되게 된다.

ML을 통해 학습해야할 값은 결과적으로 w와 b가 되게 된다.



비용 함수
임의의 w, b를 통해 훈련한 예측 값이 실제 y 값과 최대한 같기를 기대한다.
이를 위해 실제 값과 예측값의 차를 통한 함수로 cost를 구하게 된다.
손실함수라는 결과 값들의 차를 제곱하는 방식이 있긴하나, 그래프 형태 문제 때문에 비용함수를 활용한다.

비용함수
L(y^, y) = -(y * log(y예측값) + (1-y) * log(1-y의 예측값))

위와 같은 함수 형태이며
실값 y가 0인가, 1인가에 따라 예측 y^ 값의 기대 값이 달라지는 형태를 보인다.

이러한 함수의 결과 값은 훈련 샘플을 통해 구해진 값들이 얼마나 잘 예측 되었는가를 보여주며,
이러한 값들의 평균을 통해 훈련 set의 예측 정확도가 높은지 낮은지 구분할 수 있다.



매개변수 w, b의 학습
이전의 로지스틱 회귀 알고리즘과 비용 함수를 통해
비용함수의 결과 값이 가장 작게 나오는 w, b 를 구해야 한다.

w, b, 비용함수 값 j 로 구성되는 그래프는 볼록한 형태를 지니기 때문에 다양한 함수를 활용가능하다.

경사 하강법
초기 지점에서 시작해 가장 가파른 경사 방향으로 선택하여 반복해 최적화된 w, b를 찾아낸다.

b를 제거하여 단수화한 y = wx 에 대한 식으로 살펴보면
비용함수를 j(w) 라 했을 때,

w = w - a  * dj(w) / dw

여기서 a = 학습률로 이동할 단계의 정도를 의미하며
dj(w) / dw 는 j(w)의 미분계수로써 기울기를 의미함으로 이동 방향을 결정하게 된다.

이후 식에서
dj(w) / dw 는 줄여서 dw == w의 변화 동향
dj(b) / db 는 db == b의 변화 동향
을 지칭하게 된다.
로지스틱 회귀의 도함수 계산

손실함수

x1, w1, x2, w2, b 에서

z = w1x1 + w2x2 +b
a = A(z) 
L(a, y)

L은 손실함수

L에 대한 a의 도함수 da는
-y/a + (1-y)/(1-a) 이다.

L에 대한 도함수 "dz"
dL / dz = dL/da * da/dz = 위 식과 활용하면 ... = a-y

결과
L에 대한 도함수 
"dw1" = x1 * dz
"dw2" = x2 * dz

이를 바탕으로 갱신하는 방식은
@ == 학습률

w1 = w1 - @dw1
w2 = w2 - @dw2
b = b - @db
여기까지가 단일 샘플 계산



m 개의 샘플의 비용함수 J(w,b)는
J(w,b) = 1/m * L(a(i), y(1)), 즉 m개 손실 함수의 평균이다.

마찬가지로,
m개의 손실 함수의 도함수의 평균을 통해 비용함수 J(w,b)의 도함수 값을 구할 수 있음으로
이를 통해 경사 하강법을 활용할 수 있다.


이를 코드로 구현할 때, 
for문을 활용하면 2중 for문이 되게 되는데,
첫번째 for문은 1에서 m까지 세트를 동작 시키는데 필요하며,
두번째 for문은 식에서 특성값인 w1~wn 까지 손실함수 값을 연산하는데 필요하게 된다.
이러한 방식은 비효율 적이다.



벡터화
이러한 for문을 사용하지 않고 구현하기 위해 활용될 수 있는 방식
최대 300배의 속도차이가 날 수 있음

SIMD (Single Instruction Multiple Data)
cpu, gpu 둘다 적용 가능한 방식
병렬 처리 연산을 효율적으로 처리해줌

신경망이나, 로지스틱 회귀 프로그래밍에서는 가능하면 for문을 피해야 한다.
NumPy 라이브러리를 활용하면 이 방식을 활용할 수 있다.

n개의 특성을 지니는 샘플이라 가정할 때,
dw=np.zeros((n_x,1)) 로 벡터를 생성해 활용함으로써 일괄 처리가 가능하다.



완전 for문 제거 방식
X=열형태로 훈련 입력값을 넣은 행렬 (nx, m)
Z = z의 행배열, Z = np.dot(w.T, X) + b, 행렬 (1, m)
A = a의 결과 모음, a(Z) 
a는 예상값 처리 결과를 의미

결과 백터화를 통해 일괄적으로 연산이 가능하다.



전체 훈련 샘플에 대한 경사 계산
최초 샘플인 dz에 관한 행렬을 생성

dz는 dz = a-y

dZ == [ dz1, dz2 ,dz3 ... ] == (1, m)
A = [ a1, a2, a3 ... ], Y=[y1, y2, y3, ... ]
dZ = A-Y 가 됨

db = dz(i) 의 평균 == 1/m np.sum(dZ)

dw = 1/m * dZ^T = X, dZ의 곱에 대한 평균



최종적 식

Z=W^t * X + b == np.dot(w.T, X) + b
A = a(Z)
dz = A-Y
dw = 1/m * dZ^t
db = 1/m * np.sum(dZ)

w = w-adw
b = b-adb









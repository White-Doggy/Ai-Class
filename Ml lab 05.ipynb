{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification\n",
    "\"\"\"\n",
    "\n",
    "참(1), 거짓(0)으로 나눠지도록 분별하는 방식\n",
    "\n",
    "기존의 linear regression의 경우\n",
    "간단하지만, 0과 1 사이로 표현하는 binary classification에서는\n",
    "값이 너무 크고, 그래프 기울기가 작아지면 범위가 너무 넓어지는 문제가 있음\n",
    "\n",
    "그래서 1~0 범위에 표현될수 있도록 기존 linear 식인 WX에 추가적으로 변형식을 넣는데\n",
    "\n",
    "H(X) = 1 / 1+e^(-WX)\n",
    "즉, 기존 WX의 - 승을 더해 1 범위 내에서 포함되도록 고쳐주었다.\n",
    "\n",
    "\n",
    "cost 함수는 위 H(X)의 영향을 받아 굴곡이 심해지기 때문에 다른 cost 함수가 필요함\n",
    " \n",
    "y =1 / y =0의 경우에 따라\n",
    "-log(H(x)) / -log(1-H(x)) \n",
    "로 구현할 수 있지만. 번거롭다.\n",
    "\n",
    "이를 하나의 식으로 합쳐버리면\n",
    "\n",
    "C(H(x),y) = -ylog(H(x))-(1-y)log(1-H(x)) \n",
    "y 는 0 또는 1 이기에 이런 식이 가능\n",
    "\n",
    "\n",
    "이렇게 합쳐진 식은 결과적으로 이전 처럼 U 형태의 그래프를 생성하기에,\n",
    "이전에 활용하던 w를 구하던 방식을 그대로 활용 가능해진다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Lab 5 Logistic Regression Classifier\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "# shape의 형태에 주의\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
